{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnVtGxhjk+g8mqlwim39uU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pk2971/Web-scraper/blob/main/Construct_Data_List_with_sleep_Praharshita.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbkrC23Bcrnu"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "def construct_data_list(links):\n",
        "  # looping through the links\n",
        "  link_list = links\n",
        "\n",
        "  # creating a new list            \n",
        "  hansard_info_list = []\n",
        "  n=0\n",
        "\n",
        "  # for every link in the list of links\n",
        "  for link in link_list:\n",
        "\n",
        "\n",
        "    # appending the string to the link\n",
        "    # to get the entire link\n",
        "    if(n<=10)\n",
        "      string = \"https://api.parliament.uk\"\n",
        "      link = string + link\n",
        "\n",
        "    # creating a dictionary entry called hansard_info\n",
        "      hansard_info = {}\n",
        "      data = get_data(link)\n",
        "      soup = get_soup(data)\n",
        "\n",
        "    # getting the date for each link\n",
        "      hansard_info['date'] = soup.find(class_=\"sitting-day\").get_text(\" \", strip = True)\n",
        "      parser = get_parser(data)\n",
        "    # extracting the title for each link\n",
        "      hansard_info['title'] = parser.select('h1.title')[0].text.strip()\n",
        "    # extracting the conversation from the webpage for each link\n",
        "      hansard_info['conversation'] = [p.get_text(strip=True, separator='\\n') for p in soup.find_all(\"p\")]\n",
        "    # getting rid of \"noticed a typo?\"\n",
        "      hansard_info['conversation'] = hansard_info['conversation'][:-1]\n",
        "    # appending this dictionary entry the list of ditionary entries\n",
        "      hansard_info_list.append(hansard_info)\n",
        "      n+=1\n",
        "    else:\n",
        "      x=random.range(1,6)\n",
        "      print(\"Sleeping for \",x,\" seconds\")\n",
        "      time.sleep(x)\n",
        "      n=0\n",
        "  return hansard_info_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}